{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua3EE2cvY3SS"
      },
      "source": [
        "# Baseline Model\n",
        "\n",
        "## Table of Contents\n",
        "1. [Model Choice](#model-choice)\n",
        "2. [Feature Selection](#feature-selection)\n",
        "3. [Implementation](#implementation)\n",
        "4. [Evaluation](#evaluation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtPl5MidY3ST"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.svm import SVC\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO9uwUTCY3SU"
      },
      "source": [
        "## Model Choice\n",
        "\n",
        "[Explain why you've chosen a particular model as the baseline. This could be a simple statistical model or a basic machine learning model. Justify your choice.]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFuFvR8ZY3SU"
      },
      "source": [
        "## Feature Selection\n",
        "\n",
        "[Indicate which features from the dataset you will be using for the baseline model, and justify your selection.]\n",
        "\n",
        "--> Using \"DP  ALTV ASTV  Mean AC  Variance  LB  MSTV \" as these are the features with the highest discrimatory power\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MINnzmOKY3SU"
      },
      "outputs": [],
      "source": [
        "%pip install ucimlrepo\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "df = fetch_ucirepo(id=193)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = df.data.features\n",
        "y = df.data.targets\n",
        "\n",
        "# drop the first column of the y target variable\n",
        "y = y.iloc[:, 1] # this is the NSP column (Normal, suspect, pathologic)\n",
        "\n",
        "\n",
        "# Feature selection\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "#apply SelectKBest class to extract top 10 best features --> highest discriminatory power\n",
        "bestfeatures = SelectKBest(score_func=f_classif, k=10)\n",
        "fit = bestfeatures.fit(X, y)\n",
        "dfscores = pd.DataFrame(fit.scores_)\n",
        "dfcolumns = pd.DataFrame(X.columns)\n",
        "\n",
        "\n",
        "# Combine scores and column names into a single DataFrame\n",
        "featureScores = pd.concat([dfcolumns, dfscores], axis=1)\n",
        "featureScores.columns = ['Specs', 'Score']\n",
        "\n",
        "# Select the top 10 features\n",
        "X_selected = X[featureScores.nlargest(10, 'Score')['Specs']]\n",
        "# selected features with highest discriminatory power are : DP  ALTV ASTV  Mean  Mode  Median  AC  Variance  LB  MSTV\n",
        "\n",
        "# Drop the Mode and Median columns form the selected features due to hight correlation with Mean\n",
        "X_selected = X_selected.drop(['Mode', 'Median'], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# Splitting the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc62e1xDY3SV"
      },
      "source": [
        "## Implementation\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compiling and Training for Logistic Regression"
      ],
      "metadata": {
        "id": "4gSiuwiYjrUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train logistic regression with optional SMOTE\n",
        "def train_logistic_regression(X_train, y_train, X_test, apply_smote=False):\n",
        "    \"\"\"\n",
        "    Compiles and trains a logistic regression model with optional SMOTE.\n",
        "\n",
        "    Parameters:\n",
        "    X_train: Training features\n",
        "    y_train: Training labels\n",
        "    X_test: Test features (used for scaling)\n",
        "    apply_smote: Whether to apply SMOTE to balance the training data\n",
        "\n",
        "    Returns:\n",
        "    model: Trained logistic regression model\n",
        "    X_test_scaled: Scaled test features\n",
        "    \"\"\"\n",
        "    # Step 1: Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Step 2: Apply SMOTE if required\n",
        "    if apply_smote:\n",
        "        smote = SMOTE(random_state=42)\n",
        "        X_train_scaled, y_train = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "    # Step 3: Initialize logistic regression model\n",
        "    model = LogisticRegression(\n",
        "        multi_class='multinomial',\n",
        "        solver='lbfgs',\n",
        "        max_iter=1000,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Step 4: Train the model\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    return model, X_test_scaled\n",
        "\n",
        "# Function to train logistic regression with class_weight\n",
        "def train_logistic_regression_with_class_weight(X_train, y_train, X_test):\n",
        "    \"\"\"\n",
        "    Compiles and trains a logistic regression model with class weights.\n",
        "\n",
        "    Parameters:\n",
        "    X_train: Training features\n",
        "    y_train: Training labels\n",
        "    X_test: Test features (used for scaling)\n",
        "\n",
        "    Returns:\n",
        "    model: Trained logistic regression model\n",
        "    X_test_scaled: Scaled test features\n",
        "    \"\"\"\n",
        "    # Step 1: Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Step 2: Initialize logistic regression model with class_weight\n",
        "    model = LogisticRegression(\n",
        "        multi_class='multinomial',\n",
        "        solver='lbfgs',\n",
        "        max_iter=1000,\n",
        "        random_state=42,\n",
        "        class_weight='balanced'  # Automatically adjusts weights based on class frequencies\n",
        "    )\n",
        "\n",
        "    # Step 3: Train the model\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    return model, X_test_scaled\n",
        "\n",
        "\n",
        "# Train the model with SMOTE\n",
        "model_smote, X_test_smote = train_logistic_regression(X_train, y_train, X_test, apply_smote=True)\n",
        "\n",
        "# Train the model without SMOTE but with class_weight\n",
        "model_no_smote_weighted, X_test_no_smote_weighted = train_logistic_regression_with_class_weight(X_train, y_train, X_test)\n",
        "\n",
        "# Train the model without SMOTE and without class_weight\n",
        "model_no_smote, X_test_no_smote = train_logistic_regression(X_train, y_train, X_test, apply_smote=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "13dH4i9oi8MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compiling and Training for Support Vector Machine"
      ],
      "metadata": {
        "id": "DK-kyGQYtSTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_svm_with_optimized_kernel(X_train, y_train, X_test, kernel='rbf', tune_hyperparameters=False):\n",
        "    \"\"\"\n",
        "    Trains a Support Vector Machine (SVM) model with class weights and optional hyperparameter tuning.\n",
        "\n",
        "    Parameters:\n",
        "    X_train: Training features\n",
        "    y_train: Training labels\n",
        "    X_test: Test features (used for scaling)\n",
        "    kernel: Kernel type to use ('linear', 'rbf', 'poly', 'sigmoid')\n",
        "    tune_hyperparameters: Whether to perform hyperparameter tuning using GridSearchCV\n",
        "\n",
        "    Returns:\n",
        "    model: Trained SVM model\n",
        "    X_test_scaled: Scaled test features\n",
        "    \"\"\"\n",
        "    # Step 1: Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Step 2: Initialize SVM with class_weight\n",
        "    svm_model = SVC(\n",
        "        kernel=kernel,\n",
        "        probability=True,\n",
        "        random_state=42,\n",
        "        class_weight='balanced'\n",
        "    )\n",
        "\n",
        "    # Step 3: Hyperparameter tuning\n",
        "    if tune_hyperparameters:\n",
        "        # Different parameter grids depending on kernel type\n",
        "        if kernel == 'linear':\n",
        "            param_grid = {\n",
        "                'C': [0.1, 1, 10, 100],  # Only C parameter for linear kernel\n",
        "                'kernel': [kernel]\n",
        "            }\n",
        "        else:\n",
        "            param_grid = {\n",
        "                'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
        "                'gamma': [1, 0.1, 0.01, 0.001],  # Kernel coefficient\n",
        "                'kernel': [kernel]\n",
        "            }\n",
        "\n",
        "        grid_search = GridSearchCV(svm_model, param_grid, cv=3, scoring='f1_weighted', verbose=2)\n",
        "        grid_search.fit(X_train_scaled, y_train)\n",
        "        svm_model = grid_search.best_estimator_\n",
        "\n",
        "    # Step 4: Train the model\n",
        "    svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    return svm_model, X_test_scaled\n",
        "\n"
      ],
      "metadata": {
        "id": "s64lhy6dCxJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the SVM model with optimized kernel\n",
        "# Specify the kernel you want to use and whether to enable hyperparameter tuning\n",
        "svm_model, X_test_svm = train_svm_with_optimized_kernel(X_train, y_train, X_test, kernel='rbf', tune_hyperparameters=True)"
      ],
      "metadata": {
        "id": "aUS6z0OpDeOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the linear SVM model\n",
        "linear_svm_model, X_test_linear = train_svm_with_optimized_kernel(X_train, y_train, X_test, kernel='linear', tune_hyperparameters=True)"
      ],
      "metadata": {
        "id": "KKC8bzFZDCxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "IC4e38qQF0rF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics to Evaluate the Model"
      ],
      "metadata": {
        "id": "OyFiOHh-F63Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Primary Metrics\n",
        "- **Recall**: Ensure no pathological cases are missed (focus on sensitivity).\n",
        "- **F1-Score**: Balances Precision and Recall for each class.\n",
        "- **Confusion Matrix**: Analyze detailed classification errors.\n",
        "\n",
        "### Secondary Metrics\n",
        "- **Accuracy**: Provides an overall performance snapshot but is less reliable for imbalanced datasets.\n",
        "- **Precision**: Evaluate the proportion of correct positive predictions to avoid excessive false alarms.\n",
        "\n",
        "### Advanced Metric\n",
        "- **ROC-AUC**: Measure the modelâ€™s ability to distinguish between classes, useful for comparing models or optimizing thresholds.\n"
      ],
      "metadata": {
        "id": "pe2PPSCDj5BU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Logistic Regression"
      ],
      "metadata": {
        "id": "FDqBSP2vtL-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funktion zur Evaluation und Visualisierung der Modelle\n",
        "def evaluate_and_visualize_model(model, X_test, y_test, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Evaluates and visualizes the performance of a model.\n",
        "\n",
        "    Parameters:\n",
        "    model: Trained model to evaluate\n",
        "    X_test: Test features\n",
        "    y_test: True labels for the test set\n",
        "    model_name: Name of the model (e.g., \"SVM with SMOTE\")\n",
        "    \"\"\"\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Berechne Accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"\\n--- {model_name} ---\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    report = classification_report(y_test, y_pred, target_names=[\"Normal\", \"Suspect\", \"Pathologic\"], output_dict=True)\n",
        "    df_report = pd.DataFrame(report).transpose()\n",
        "    print(df_report)\n",
        "\n",
        "    # Visualisiere den Klassifikationsbericht als Heatmap\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(df_report.iloc[:-3, :3], annot=True, cmap=\"Blues\", fmt=\".2f\", linewidths=0.5)\n",
        "    plt.title(f\"Precision, Recall, and F1-Score ({model_name} Heatmap Table)\")\n",
        "    plt.xlabel(\"Metrics\")\n",
        "    plt.ylabel(\"Classes\")\n",
        "    plt.show()\n",
        "\n",
        "    # Confusion Matrix\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[\"Normal\", \"Suspect\", \"Pathologic\"])\n",
        "    disp.plot(cmap=\"viridis\")\n",
        "    plt.title(f\"Confusion Matrix ({model_name})\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot ROC curves if probabilities are enabled\n",
        "    if hasattr(model, \"predict_proba\") and callable(model.predict_proba):\n",
        "        y_proba = model.predict_proba(X_test)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "\n",
        "        # Compute ROC curve and AUC for each class\n",
        "        for i, class_label in enumerate([\"Normal\", \"Suspect\", \"Pathologic\"]):\n",
        "            fpr, tpr, _ = roc_curve(y_test == i + 1, y_proba[:, i])\n",
        "            auc = roc_auc_score(y_test == i + 1, y_proba[:, i])\n",
        "            plt.plot(fpr, tpr, label=f\"Class {class_label} (AUC = {auc:.2f})\")\n",
        "\n",
        "        # Add diagonal line for random guess\n",
        "        plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
        "\n",
        "        plt.title(f\"ROC Curves for Each Class ({model_name})\")\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Probability estimates are not available for {model_name}.\")"
      ],
      "metadata": {
        "id": "9T-svDJOmizp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Logistic Regression with SMOTE"
      ],
      "metadata": {
        "id": "U404J7myGtfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate and visualize the model trained with SMOTE\n",
        "evaluate_and_visualize_model(model_smote, X_test_smote, y_test, model_name=\"Logistic Regression with SMOTE\")\n"
      ],
      "metadata": {
        "id": "p2RD3sVzpbDq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Logistic Regression with class_weight"
      ],
      "metadata": {
        "id": "pAq1FoAmG0tB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate and visualize the model trained with class_weight\n",
        "evaluate_and_visualize_model(model_no_smote_weighted, X_test_no_smote_weighted, y_test, model_name=\"Logistic Regression with class_weight\")"
      ],
      "metadata": {
        "id": "X4xhuT6grcvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Logistic Regression without SMOTE and withou class_weight"
      ],
      "metadata": {
        "id": "-oqgulgSG4wj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate and visualize the model trained without class_weight and without SMOTE\n",
        "evaluate_and_visualize_model(model_no_smote, X_test_no_smote, y_test, model_name=\"Logistic Regression without SMOTE and without class_weight\")"
      ],
      "metadata": {
        "id": "8vpVX77MsFa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Support Vector Machine"
      ],
      "metadata": {
        "id": "cTkordBQtaeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Support Vectoe Machine with Non-Linear Kernel (rbf)"
      ],
      "metadata": {
        "id": "gsfxi2LaHFLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the SVM model with the non-linear kernel\n",
        "evaluate_and_visualize_model(svm_model, X_test_svm, y_test, model_name=\"SVM with Optimized Kernel\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WMg4NOhvqo_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the linear SVM model\n",
        "evaluate_and_visualize_model(linear_svm_model, X_test_linear, y_test, model_name=\"SVM with Linear Kernel\")"
      ],
      "metadata": {
        "id": "zRTNjJLWDVR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing Logistic Regression (with class_weight) and Support Vector Machine (kernel='rbf')"
      ],
      "metadata": {
        "id": "6oti8sTHwrV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_models(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Trains and compares SVM and Logistic Regression models.\n",
        "    Returns the comparison metrics and creates visualization plots.\n",
        "    \"\"\"\n",
        "    # Train Logistic Regression with class weights\n",
        "    logreg_model, X_test_logreg = train_logistic_regression_with_class_weight(X_train, y_train, X_test)\n",
        "\n",
        "    # Train SVM with RBF kernel\n",
        "    svm_model, X_test_svm = train_svm_with_optimized_kernel(\n",
        "        X_train, y_train, X_test,\n",
        "        kernel='rbf',\n",
        "        tune_hyperparameters=False\n",
        "    )\n",
        "\n",
        "    # Get predictions\n",
        "    y_pred_logreg = logreg_model.predict(X_test_logreg)\n",
        "    y_pred_svm = svm_model.predict(X_test_svm)\n",
        "\n",
        "    # Get classification reports\n",
        "    report_logreg = classification_report(y_test, y_pred_logreg,\n",
        "                                        target_names=[\"Normal\", \"Suspect\", \"Pathologic\"],\n",
        "                                        output_dict=True)\n",
        "    report_svm = classification_report(y_test, y_pred_svm,\n",
        "                                     target_names=[\"Normal\", \"Suspect\", \"Pathologic\"],\n",
        "                                     output_dict=True)\n",
        "\n",
        "    # Create comparison DataFrame with consistent naming\n",
        "    comparison_data = {\n",
        "        'precision_LogReg': [report_logreg[label]['precision'] for label in [\"Normal\", \"Suspect\", \"Pathologic\"]],\n",
        "        'recall_LogReg': [report_logreg[label]['recall'] for label in [\"Normal\", \"Suspect\", \"Pathologic\"]],\n",
        "        'f1-score_LogReg': [report_logreg[label]['f1-score'] for label in [\"Normal\", \"Suspect\", \"Pathologic\"]],\n",
        "        'precision_SVM': [report_svm[label]['precision'] for label in [\"Normal\", \"Suspect\", \"Pathologic\"]],\n",
        "        'recall_SVM': [report_svm[label]['recall'] for label in [\"Normal\", \"Suspect\", \"Pathologic\"]],\n",
        "        'f1-score_SVM': [report_svm[label]['f1-score'] for label in [\"Normal\", \"Suspect\", \"Pathologic\"]]\n",
        "    }\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison_data,\n",
        "                               index=[\"Normal\", \"Suspect\", \"Pathologic\"])\n",
        "\n",
        "    # Create visualization\n",
        "    metrics = [\"Precision\", \"Recall\", \"F1-Score\"]\n",
        "    classes = comparison_df.index\n",
        "    x = np.arange(len(classes))\n",
        "    width = 0.35\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        # Handle column names correctly\n",
        "        if metric == \"F1-Score\":\n",
        "            logreg_values = comparison_df['f1-score_LogReg']\n",
        "            svm_values = comparison_df['f1-score_SVM']\n",
        "        else:\n",
        "            logreg_values = comparison_df[f'{metric.lower()}_LogReg']\n",
        "            svm_values = comparison_df[f'{metric.lower()}_SVM']\n",
        "\n",
        "        ax = axes[i]\n",
        "        rects1 = ax.bar(x - width/2, logreg_values, width, label='Logistic Regression',\n",
        "                       color='skyblue', alpha=0.8)\n",
        "        rects2 = ax.bar(x + width/2, svm_values, width, label='SVM',\n",
        "                       color='lightcoral', alpha=0.8)\n",
        "\n",
        "        ax.set_title(metric)\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(classes, rotation=45, ha='right')\n",
        "        ax.set_ylabel(\"Score\")\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.legend()\n",
        "\n",
        "        # Add value labels on bars\n",
        "        def autolabel(rects):\n",
        "            for rect in rects:\n",
        "                height = rect.get_height()\n",
        "                ax.annotate(f'{height:.2f}',\n",
        "                          xy=(rect.get_x() + rect.get_width()/2, height),\n",
        "                          xytext=(0, 3),\n",
        "                          textcoords=\"offset points\",\n",
        "                          ha='center', va='bottom')\n",
        "\n",
        "        autolabel(rects1)\n",
        "        autolabel(rects2)\n",
        "\n",
        "    fig.suptitle(\"Comparison of Metrics: Logistic Regression vs. SVM\", fontsize=16)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\nModel Comparison Summary:\")\n",
        "    print(\"\\nLogistic Regression Weighted Average Metrics:\")\n",
        "    print(f\"Precision: {report_logreg['weighted avg']['precision']:.3f}\")\n",
        "    print(f\"Recall: {report_logreg['weighted avg']['recall']:.3f}\")\n",
        "    print(f\"F1-Score: {report_logreg['weighted avg']['f1-score']:.3f}\")\n",
        "\n",
        "    print(\"\\nSVM Weighted Average Metrics:\")\n",
        "    print(f\"Precision: {report_svm['weighted avg']['precision']:.3f}\")\n",
        "    print(f\"Recall: {report_svm['weighted avg']['recall']:.3f}\")\n",
        "    print(f\"F1-Score: {report_svm['weighted avg']['f1-score']:.3f}\")\n",
        "\n",
        "    return comparison_df\n",
        "\n"
      ],
      "metadata": {
        "id": "_WxQDzxPtVQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the comparison function\n",
        "comparison_results = compare_models(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "ObEdRv6mtWi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysing the Importance of the Included Features"
      ],
      "metadata": {
        "id": "EIMFUsp29Kc_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "HjZqRHWq-7VY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature names\n",
        "feature_names = ['DP', 'ALTV', 'ASTV', 'Mean', 'AC', 'Variance', 'LB', 'MSTV']\n",
        "\n",
        "def analyze_features(model, feature_names):\n",
        "    \"\"\"\n",
        "    Analyzes feature importance from a trained logistic regression model.\n",
        "\n",
        "    Parameters:\n",
        "    model: Trained logistic regression model\n",
        "    feature_names: List of feature names\n",
        "\n",
        "    Returns:\n",
        "    coef_df: DataFrame with coefficients per class\n",
        "    importance_df: DataFrame with overall feature importance\n",
        "    \"\"\"\n",
        "    # Get coefficients per class\n",
        "    coef_df = pd.DataFrame(\n",
        "        model.coef_,\n",
        "        columns=feature_names,\n",
        "        index=['Normal vs Rest', 'Suspect vs Rest', 'Pathologic vs Rest']\n",
        "    )\n",
        "\n",
        "    # Calculate overall feature importance\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Absolute Importance': np.abs(model.coef_).mean(axis=0)\n",
        "    }).sort_values('Absolute Importance', ascending=False)\n",
        "\n",
        "    # Visualizations\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # 1. Feature coefficients heatmap\n",
        "    plt.subplot(2, 1, 1)\n",
        "    sns.heatmap(coef_df, cmap='RdBu', center=0, annot=True, fmt='.2f',\n",
        "                cbar_kws={'label': 'Coefficient Value'})\n",
        "    plt.title('Feature Influence per Class')\n",
        "    plt.xlabel('Features')\n",
        "    plt.ylabel('Classes')\n",
        "\n",
        "    # 2. Overall feature importance\n",
        "    plt.subplot(2, 1, 2)\n",
        "    sns.barplot(data=importance_df, x='Absolute Importance', y='Feature')\n",
        "    plt.title('Feature Importance Ranking')\n",
        "    plt.xlabel('Absolute Coefficient Value (averaged across classes)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print feature importance ranking\n",
        "    print(\"\\nFeature Importance Ranking:\")\n",
        "    for idx, row in importance_df.iterrows():\n",
        "        print(f\"{row['Feature']}: {row['Absolute Importance']:.4f}\")\n",
        "\n",
        "    # Print class-specific feature influence\n",
        "    print(\"\\nFeature Influence per Class:\")\n",
        "    for i, class_name in enumerate(['Normal', 'Suspect', 'Pathologic']):\n",
        "        coef = model.coef_[i]\n",
        "        feat_importance = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            'Coefficient': coef\n",
        "        }).sort_values('Coefficient', key=abs, ascending=False)\n",
        "\n",
        "        print(f\"\\n{class_name}:\")\n",
        "        print(\"Top positive indicators:\")\n",
        "        positive = feat_importance[feat_importance['Coefficient'] > 0].head(3)\n",
        "        for _, row in positive.iterrows():\n",
        "            print(f\"{row['Feature']}: +{row['Coefficient']:.4f}\")\n",
        "\n",
        "        print(\"Top negative indicators:\")\n",
        "        negative = feat_importance[feat_importance['Coefficient'] < 0].head(3)\n",
        "        for _, row in negative.iterrows():\n",
        "            print(f\"{row['Feature']}: {row['Coefficient']:.4f}\")\n",
        "\n",
        "    return coef_df, importance_df\n",
        "\n",
        "# Run analysis for weighted model\n",
        "coef_df, importance_df = analyze_features(model_no_smote_weighted, feature_names)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vM46ofOA9Jre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machine (Linear)"
      ],
      "metadata": {
        "id": "P3HHPRc3Aa1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_linear_svm_features(model, feature_names):\n",
        "    \"\"\"\n",
        "    Analyzes feature importance from a trained linear SVM model.\n",
        "\n",
        "    Parameters:\n",
        "    model: Trained linear SVM model\n",
        "    feature_names: List of feature names\n",
        "\n",
        "    Returns:\n",
        "    coef_df: DataFrame with coefficients per class\n",
        "    importance_df: DataFrame with overall feature importance\n",
        "    \"\"\"\n",
        "    # Get coefficients per class\n",
        "    coef_df = pd.DataFrame(\n",
        "        model.coef_,\n",
        "        columns=feature_names,\n",
        "        index=['Normal vs Rest', 'Suspect vs Rest', 'Pathologic vs Rest']\n",
        "    )\n",
        "\n",
        "    # Calculate overall feature importance\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Absolute Importance': np.abs(model.coef_).mean(axis=0)\n",
        "    }).sort_values('Absolute Importance', ascending=False)\n",
        "\n",
        "    # Visualizations\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # 1. Feature coefficients heatmap\n",
        "    plt.subplot(2, 1, 1)\n",
        "    sns.heatmap(coef_df, cmap='RdBu', center=0, annot=True, fmt='.2f',\n",
        "                cbar_kws={'label': 'Coefficient Value'})\n",
        "    plt.title('Linear SVM Coefficients by Class')\n",
        "    plt.xlabel('Features')\n",
        "    plt.ylabel('Classes')\n",
        "\n",
        "    # 2. Overall feature importance\n",
        "    plt.subplot(2, 1, 2)\n",
        "    sns.barplot(data=importance_df, x='Absolute Importance', y='Feature')\n",
        "    plt.title('Feature Importance Ranking')\n",
        "    plt.xlabel('Absolute Coefficient Value (averaged across classes)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print feature importance ranking\n",
        "    print(\"\\nFeature Importance Ranking:\")\n",
        "    for idx, row in importance_df.iterrows():\n",
        "        print(f\"{row['Feature']}: {row['Absolute Importance']:.4f}\")\n",
        "\n",
        "    # Print class-specific feature influence\n",
        "    print(\"\\nFeature Influence per Class:\")\n",
        "    for i, class_name in enumerate(['Normal', 'Suspect', 'Pathologic']):\n",
        "        coef = model.coef_[i]\n",
        "        feat_importance = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            'Coefficient': coef\n",
        "        }).sort_values('Coefficient', key=abs, ascending=False)\n",
        "\n",
        "        print(f\"\\n{class_name}:\")\n",
        "        print(\"Top positive indicators:\")\n",
        "        positive = feat_importance[feat_importance['Coefficient'] > 0].head(3)\n",
        "        for _, row in positive.iterrows():\n",
        "            print(f\"{row['Feature']}: +{row['Coefficient']:.4f}\")\n",
        "\n",
        "        print(\"Top negative indicators:\")\n",
        "        negative = feat_importance[feat_importance['Coefficient'] < 0].head(3)\n",
        "        for _, row in negative.iterrows():\n",
        "            print(f\"{row['Feature']}: {row['Coefficient']:.4f}\")\n",
        "\n",
        "    return coef_df, importance_df\n",
        "\n",
        "# Define feature names\n",
        "feature_names = ['DP', 'ALTV', 'ASTV', 'Mean', 'AC', 'Variance', 'LB', 'MSTV']\n",
        "\n",
        "# Run the analysis\n",
        "coefficients, importance = analyze_linear_svm_features(linear_svm_model, feature_names)\n"
      ],
      "metadata": {
        "id": "RankkLleDuuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing Feature Importance for Logistic Regression and Support Vector Machine (Linear)"
      ],
      "metadata": {
        "id": "FXVdjTbDFBJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_feature_importance(logreg_model, svm_model, feature_names):\n",
        "    \"\"\"\n",
        "    Compares feature importance between logistic regression and linear SVM.\n",
        "\n",
        "    Parameters:\n",
        "    logreg_model: Trained logistic regression model\n",
        "    svm_model: Trained linear SVM model\n",
        "    feature_names: List of feature names\n",
        "    \"\"\"\n",
        "    # Get coefficients for both models\n",
        "    logreg_coef = pd.DataFrame(\n",
        "        logreg_model.coef_,\n",
        "        columns=feature_names,\n",
        "        index=['Normal vs Rest', 'Suspect vs Rest', 'Pathologic vs Rest']\n",
        "    )\n",
        "\n",
        "    svm_coef = pd.DataFrame(\n",
        "        svm_model.coef_,\n",
        "        columns=feature_names,\n",
        "        index=['Normal vs Rest', 'Suspect vs Rest', 'Pathologic vs Rest']\n",
        "    )\n",
        "\n",
        "    # Calculate overall importance for both models\n",
        "    logreg_importance = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Absolute Importance': np.abs(logreg_model.coef_).mean(axis=0),\n",
        "        'Model': 'Logistic Regression'\n",
        "    }).sort_values('Absolute Importance', ascending=False)\n",
        "\n",
        "    svm_importance = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Absolute Importance': np.abs(svm_model.coef_).mean(axis=0),\n",
        "        'Model': 'Linear SVM'\n",
        "    }).sort_values('Absolute Importance', ascending=False)\n",
        "\n",
        "    # Combine importance scores\n",
        "    combined_importance = pd.concat([logreg_importance, svm_importance])\n",
        "\n",
        "    # Visualizations\n",
        "    plt.figure(figsize=(15, 15))\n",
        "\n",
        "    # 1. Coefficient comparison heatmap\n",
        "    plt.subplot(2, 1, 1)\n",
        "    comparison_df = pd.DataFrame({\n",
        "        'LogReg': np.abs(logreg_model.coef_).mean(axis=0),\n",
        "        'SVM': np.abs(svm_model.coef_).mean(axis=0)\n",
        "    }, index=feature_names)\n",
        "\n",
        "    sns.heatmap(comparison_df, cmap='YlOrRd', annot=True, fmt='.2f',\n",
        "                cbar_kws={'label': 'Absolute Coefficient Value'})\n",
        "    plt.title('Feature Importance Comparison: Logistic Regression vs Linear SVM')\n",
        "\n",
        "    # 2. Side-by-side barplot\n",
        "    plt.subplot(2, 1, 2)\n",
        "    sns.barplot(data=combined_importance, x='Absolute Importance', y='Feature',\n",
        "                hue='Model', palette=['skyblue', 'lightcoral'])\n",
        "    plt.title('Feature Importance Ranking Comparison')\n",
        "    plt.xlabel('Absolute Coefficient Value (averaged across classes)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print comparison\n",
        "    print(\"\\nFeature Importance Comparison:\")\n",
        "    comparison = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'LogReg_Importance': np.abs(logreg_model.coef_).mean(axis=0),\n",
        "        'SVM_Importance': np.abs(svm_model.coef_).mean(axis=0)\n",
        "    }).sort_values('LogReg_Importance', ascending=False)\n",
        "\n",
        "    for idx, row in comparison.iterrows():\n",
        "        print(f\"\\n{row['Feature']}:\")\n",
        "        print(f\"  Logistic Regression: {row['LogReg_Importance']:.4f}\")\n",
        "        print(f\"  Linear SVM: {row['SVM_Importance']:.4f}\")\n",
        "        print(f\"  Difference: {abs(row['LogReg_Importance'] - row['SVM_Importance']):.4f}\")\n",
        "\n",
        "# Define feature names\n",
        "feature_names = ['DP', 'ALTV', 'ASTV', 'Mean', 'AC', 'Variance', 'LB', 'MSTV']\n",
        "\n",
        "# Compare models\n",
        "compare_feature_importance(model_no_smote_weighted, linear_svm_model, feature_names)\n",
        "\n"
      ],
      "metadata": {
        "id": "bsw8OzVSEVrg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}