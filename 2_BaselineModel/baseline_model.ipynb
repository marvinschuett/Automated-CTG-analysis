{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua3EE2cvY3SS"
      },
      "source": [
        "# Baseline Model\n",
        "\n",
        "## Table of Contents\n",
        "1. [Model Choice](#model-choice)\n",
        "2. [Feature Selection](#feature-selection)\n",
        "3. [Implementation](#implementation)\n",
        "4. [Evaluation](#evaluation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtPl5MidY3ST"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "# Import your chosen baseline model\n",
        "# Example: from sklearn.linear_model import LogisticRegression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO9uwUTCY3SU"
      },
      "source": [
        "## Model Choice\n",
        "\n",
        "[Explain why you've chosen a particular model as the baseline. This could be a simple statistical model or a basic machine learning model. Justify your choice.]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFuFvR8ZY3SU"
      },
      "source": [
        "## Feature Selection\n",
        "\n",
        "[Indicate which features from the dataset you will be using for the baseline model, and justify your selection.]\n",
        "\n",
        "--> Using \"DP  ALTV ASTV  Mean AC  Variance  LB  MSTV \" as these are the features with the highest discrimatory power\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MINnzmOKY3SU"
      },
      "outputs": [],
      "source": [
        "%pip install ucimlrepo\n",
        "\n",
        "# Load the dataset\n",
        "import pandas as pd\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "df = fetch_ucirepo(id=193)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = df.data.features\n",
        "y = df.data.targets\n",
        "\n",
        "# drop the first column of the y target variable\n",
        "y = y.iloc[:, 1] # this is the NSP column (Normal, suspect, pathologic)\n",
        "\n",
        "\n",
        "# Feature selection\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "#apply SelectKBest class to extract top 10 best features --> highest discriminatory power\n",
        "bestfeatures = SelectKBest(score_func=f_classif, k=10)\n",
        "fit = bestfeatures.fit(X, y)\n",
        "dfscores = pd.DataFrame(fit.scores_)\n",
        "dfcolumns = pd.DataFrame(X.columns)\n",
        "\n",
        "\n",
        "# Combine scores and column names into a single DataFrame\n",
        "featureScores = pd.concat([dfcolumns, dfscores], axis=1)\n",
        "featureScores.columns = ['Specs', 'Score']\n",
        "\n",
        "# Select the top 10 features\n",
        "X_selected = X[featureScores.nlargest(10, 'Score')['Specs']]\n",
        "# selected features with highest discriminatory power are : DP  ALTV ASTV  Mean  Mode  Median  AC  Variance  LB  MSTV\n",
        "\n",
        "# Drop the Mode and Median columns form the selected features due to hight correlation with Mean\n",
        "X_selected = X_selected.drop(['Mode', 'Median'], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# Splitting the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc62e1xDY3SV"
      },
      "source": [
        "## Implementation\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "0JPpeyG-dpeX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZfDR8rEY3SV"
      },
      "outputs": [],
      "source": [
        "# Initialize and train the baseline model\n",
        "# Example for a classification problem using Logistic Regression\n",
        "# model = LogisticRegression()\n",
        "# model.fit(X_train, y_train)\n",
        "\n",
        "# Your implementation code here\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_selected)  # Assuming X_selected contains your features\n",
        "\n",
        "# Step 2: Apply SMOTE to balance the dataset\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
        "\n",
        "# Step 3: Train-test split with the resampled data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Initialize logistic regression model\n",
        "model = LogisticRegression(\n",
        "    multi_class='multinomial',\n",
        "    solver='lbfgs',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 5: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model accuracy after applying SMOTE:\", accuracy)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 8: Plot the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "custom_labels = ['Normal=1', 'Suspect=2', 'Pathologic=3']\n",
        "\n",
        "disp = ConfusionMatrixDisplay(conf_matrix, display_labels=custom_labels)\n",
        "disp.plot(cmap=\"viridis\")\n",
        "\n",
        "# using a simple multiiclass classification model using tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cVibXVtlcb0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hz5zkow0cj79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic regression with SMOTE"
      ],
      "metadata": {
        "id": "DAiNoM22dvlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_selected)  # Assuming X_selected contains your features\n",
        "\n",
        "# Apply SMOTE to balance the dataset\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
        "\n",
        "# Train-test split with the resampled data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize logistic regression model\n",
        "model = LogisticRegression(\n",
        "    multi_class='multinomial',\n",
        "    solver='lbfgs',\n",
        "    max_iter=1000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "uFAuzqNeeItn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Support Vector Machine"
      ],
      "metadata": {
        "id": "DK-kyGQYtSTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Initialize the Support Vector Machine model\n",
        "svm_model = SVC(\n",
        "    kernel='linear',  # You can try other kernels like 'rbf', 'poly', etc.\n",
        "    probability=True,  # Enable if you need probabilities\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the SVM model\n",
        "svm_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "51VxU9pwt-6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14bW_giSY3SV"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "[Clearly state what metrics you will use to evaluate the model's performance. These metrics will serve as a starting point for evaluating more complex models later on.]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Metrics to Evaluate the Model\n",
        "\n",
        "### Primary Metrics\n",
        "- **Recall**: Ensure no pathological cases are missed (focus on sensitivity).\n",
        "- **F1-Score**: Balances Precision and Recall for each class.\n",
        "- **Confusion Matrix**: Analyze detailed classification errors.\n",
        "\n",
        "### Secondary Metrics\n",
        "- **Accuracy**: Provides an overall performance snapshot but is less reliable for imbalanced datasets.\n",
        "- **Precision**: Evaluate the proportion of correct positive predictions to avoid excessive false alarms.\n",
        "\n",
        "### Advanced Metric\n",
        "- **ROC-AUC**: Measure the modelâ€™s ability to distinguish between classes, useful for comparing models or optimizing thresholds.\n"
      ],
      "metadata": {
        "id": "pe2PPSCDj5BU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ku6AHkpY3SV"
      },
      "outputs": [],
      "source": [
        "# Evaluate the baseline model\n",
        "# Example for a classification problem\n",
        "# y_pred = model.predict(X_test)\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# For a regression problem, you might use:\n",
        "# mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Your evaluation code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZLGKl7EytHOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation logistic regression"
      ],
      "metadata": {
        "id": "FDqBSP2vtL-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model accuracy after applying SMOTE:\", accuracy)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "p2RD3sVzpbDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert classification report to DataFrame\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Extract metrics from classification report\n",
        "report = classification_report(y_test, y_pred, target_names=[\"Normal\", \"Suspect\", \"Pathologic\"], output_dict=True)\n",
        "df_report = pd.DataFrame(report).transpose()\n",
        "\n",
        "# Display as a table\n",
        "print(\"\\nClassification Report as Table:\")\n",
        "print(df_report)\n",
        "\n",
        "# Optional: Visualize the table in a cleaner format (if using Jupyter or Colab)\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df_report.iloc[:-3, :3], annot=True, cmap=\"Blues\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Precision, Recall, and F1-Score (Heatmap Table)\")\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel(\"Classes\")\n",
        "plt.show()\n",
        "\n",
        "# Plot the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(conf_matrix, display_labels=model.classes_)\n",
        "disp.plot(cmap=\"viridis\")\n"
      ],
      "metadata": {
        "id": "X4xhuT6grcvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# Compute ROC curve and AUC for each class\n",
        "y_proba = model.predict_proba(X_test)\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i, class_label in enumerate(model.classes_):\n",
        "    fpr, tpr, _ = roc_curve(y_test == class_label, y_proba[:, i])\n",
        "    auc = roc_auc_score(y_test == class_label, y_proba[:, i])\n",
        "    plt.plot(fpr, tpr, label=f\"Class {class_label} (AUC = {auc:.2f})\")\n",
        "\n",
        "# Add diagonal line for random guess\n",
        "plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
        "\n",
        "plt.title(\"ROC Curves for Each Class\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8vpVX77MsFa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Support Vector Machine"
      ],
      "metadata": {
        "id": "cTkordBQtaeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
        "disp.plot()\n",
        "\n",
        "# Convert classification report to DataFrame for SVM\n",
        "report_svm = classification_report(y_test, y_pred, target_names=[\"Normal\", \"Suspect\", \"Pathologic\"], output_dict=True)\n",
        "df_report_svm = pd.DataFrame(report_svm).transpose()\n",
        "\n",
        "# Display as a table\n",
        "print(\"\\nClassification Report as Table (SVM):\")\n",
        "print(df_report_svm)\n",
        "\n",
        "# Optional: Visualize the table in a cleaner format\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df_report_svm.iloc[:-3, :3], annot=True, cmap=\"Blues\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Precision, Recall, and F1-Score (SVM Heatmap Table)\")\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel(\"Classes\")\n",
        "plt.show()\n",
        "\n",
        "# Plot the confusion matrix for SVM\n",
        "conf_matrix_svm = confusion_matrix(y_test, y_pred)\n",
        "disp_svm = ConfusionMatrixDisplay(conf_matrix_svm, display_labels=svm_model.classes_)\n",
        "disp_svm.plot(cmap=\"viridis\")\n",
        "plt.title(\"Confusion Matrix (SVM)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mpL-LpStut7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Check if probabilities are enabled\n",
        "if svm_model.probability:\n",
        "    y_proba_svm = svm_model.predict_proba(X_test)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Compute ROC curve and AUC for each class\n",
        "    for i, class_label in enumerate(svm_model.classes_):\n",
        "        fpr, tpr, _ = roc_curve(y_test == class_label, y_proba_svm[:, i])\n",
        "        auc = roc_auc_score(y_test == class_label, y_proba_svm[:, i])\n",
        "        plt.plot(fpr, tpr, label=f\"Class {class_label} (AUC = {auc:.2f})\")\n",
        "\n",
        "    # Add diagonal line for random guess\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
        "\n",
        "    plt.title(\"ROC Curves for Each Class (SVM)\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Probability estimates are not available. Set `probability=True` when initializing the SVM model.\")\n"
      ],
      "metadata": {
        "id": "s_JJlk36vNHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing logistic regression and support vector machine"
      ],
      "metadata": {
        "id": "6oti8sTHwrV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Prepare data for plotting\n",
        "metrics = [\"Precision\", \"Recall\", \"F1-Score\"]\n",
        "classes = comparison_df_plot.index\n",
        "x = np.arange(len(classes))  # Label locations\n",
        "width = 0.35  # Width of the bars\n",
        "\n",
        "# Plot each metric separately\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
        "\n",
        "for i, metric in enumerate(metrics):\n",
        "    logreg_values = comparison_df_plot[f\"{metric.lower()}_LogReg\"]\n",
        "    svm_values = comparison_df_plot[f\"{metric.lower()}_SVM\"]\n",
        "\n",
        "    ax = axes[i]\n",
        "    ax.bar(x - width / 2, logreg_values, width, label='Logistic Regression')\n",
        "    ax.bar(x + width / 2, svm_values, width, label='SVM')\n",
        "\n",
        "    ax.set_title(metric)\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(classes, rotation=45, ha='right')\n",
        "    ax.set_ylabel(\"Score\")\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.legend()\n",
        "\n",
        "fig.suptitle(\"Comparison of Metrics: Logistic Regression vs. SVM\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "LSzukuq7vg0m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}