{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing and Binary Classification with Keras\n",
    "\n",
    "## Objective\n",
    "In this week's exercise, you will:\n",
    "1. Learn how to image preprocessing in keras.\n",
    "2. Build and train a multilayer neural network for binary classification on a real-world dataset of cats and dogs.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Import Libraries\n",
    "Let's start by importing the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.9.7-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: absl-py in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow-datasets) (2.1.0)\n",
      "Collecting click (from tensorflow-datasets)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting dm-tree (from tensorflow-datasets)\n",
      "  Downloading dm_tree-0.1.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
      "Collecting immutabledict (from tensorflow-datasets)\n",
      "  Downloading immutabledict-4.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow-datasets) (2.0.2)\n",
      "Collecting promise (from tensorflow-datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.20 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow-datasets) (5.28.3)\n",
      "Requirement already satisfied: psutil in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow-datasets) (6.0.0)\n",
      "Collecting pyarrow (from tensorflow-datasets)\n",
      "  Downloading pyarrow-18.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/codespace/.local/lib/python3.12/site-packages (from tensorflow-datasets) (2.32.3)\n",
      "Collecting simple-parsing (from tensorflow-datasets)\n",
      "  Downloading simple_parsing-0.1.6-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets)\n",
      "  Downloading tensorflow_metadata-1.16.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: termcolor in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow-datasets) (2.5.0)\n",
      "Collecting toml (from tensorflow-datasets)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting tqdm (from tensorflow-datasets)\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorflow-datasets) (1.16.0)\n",
      "Collecting array-record>=0.5.0 (from tensorflow-datasets)\n",
      "  Downloading array_record-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (699 bytes)\n",
      "Collecting etils>=1.9.1 (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading etils-1.10.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (2024.2.0)\n",
      "Collecting importlib_resources (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: typing_extensions in /home/codespace/.local/lib/python3.12/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets) (4.9.0)\n",
      "Collecting zipp (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets)\n",
      "  Downloading zipp-3.20.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.19.0->tensorflow-datasets) (2024.8.30)\n",
      "Requirement already satisfied: six in /home/codespace/.local/lib/python3.12/site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
      "Collecting docstring-parser<1.0,>=0.15 (from simple-parsing->tensorflow-datasets)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.56.4 (from tensorflow-metadata->tensorflow-datasets)\n",
      "  Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading tensorflow_datasets-4.9.7-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading array_record-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading etils-1.10.0-py3-none-any.whl (164 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading dm_tree-0.1.8-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "Downloading immutabledict-4.2.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pyarrow-18.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading simple_parsing-0.1.6-py3-none-any.whl (112 kB)\n",
      "Downloading tensorflow_metadata-1.16.1-py3-none-any.whl (28 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "Downloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Downloading zipp-3.20.2-py3-none-any.whl (9.2 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21483 sha256=dd2b59a0558f5d9dab8b7be9a06fdab3aa55502e9a116bc3d451487d5bf31912\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/e7/e6/28/864bdfee5339dbd6ddcb5a186286a8e217648ec198bdf0097d\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, zipp, tqdm, toml, pyarrow, promise, importlib_resources, immutabledict, googleapis-common-protos, etils, docstring-parser, click, tensorflow-metadata, simple-parsing, array-record, tensorflow-datasets\n",
      "\u001b[33m  WARNING: The script tqdm is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script tfds is installed in '/usr/local/python/3.12.1/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed array-record-0.5.1 click-8.1.7 dm-tree-0.1.8 docstring-parser-0.16 etils-1.10.0 googleapis-common-protos-1.65.0 immutabledict-4.2.0 importlib_resources-6.4.5 promise-2.3 pyarrow-18.0.0 simple-parsing-0.1.6 tensorflow-datasets-4.9.7 tensorflow-metadata-1.16.1 toml-0.10.2 tqdm-4.67.0 zipp-3.20.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "%pip install tensorflow-datasets\n",
    "import tensorflow_datasets as tfds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Load and Preprocess the Data\n",
    "We will use the Keras `ImageDataGenerator` for image augmentation and preprocessing.\n",
    "First, unzip the uploaded dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 16:47:58.647247: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /home/codespace/tensorflow_datasets/cats_vs_dogs/4.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Dl Size...: 100%|██████████| 786/786 [00:18<00:00, 42.14 MiB/s]]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:18<00:00, 18.65s/ url]\n",
      "Generating splits...:   0%|          | 0/1 [00:00<?, ? splits/s]2024-11-07 16:48:19.291185: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "Corrupt JPEG data: 239 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 214 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 128 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 99 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 1153 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 396 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 228 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 162 extraneous bytes before marker 0xd9\n",
      "Warning: unknown JFIF revision number 0.00\n",
      "Corrupt JPEG data: 1403 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 252 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 2226 extraneous bytes before marker 0xd9\n",
      "Corrupt JPEG data: 65 extraneous bytes before marker 0xd9\n",
      "WARNING:absl:1738 images were corrupted and were skipped\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset cats_vs_dogs downloaded and prepared to /home/codespace/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# download the cats_vs_dogs dataset from  tensorflow_datasets \n",
    "\n",
    "dataset, info = tfds.load('cats_vs_dogs', with_info=True, as_supervised=True)\n",
    "dataset_size = info.splits['train'].num_examples\n",
    "class_names = info.features['label'].names\n",
    "n_classes = info.features['label'].num_classes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Learn about undersampling and implement it\n",
    "Research online what undersampling and random undersampling is. It is a very powerful technique used often in machine Learning. Find out when it is used and undersample your dataset using \"random undersampling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undersample your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Set Up ImageDataGenerator (or well more specifically the new version)\n",
    "Were Sorry - the videos from the coursera course are sometimes not the most up to date. In this case the 'ImageDataGenerator' function is deprecated (look here https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) and will be removed in the future versions. The concept behind the new reccomended function is very similar though.\n",
    "The new reccomendation is loading images with tf.keras.utils.image_dataset_from_directory and transforming the output tf.data.Dataset with preprocessing layers.\n",
    "\n",
    "You may use Chat GPT for this task and you can also check the following tutorials <br>\n",
    "https://www.tensorflow.org/tutorials/load_data/images <br>\n",
    "https://www.tensorflow.org/tutorials/load_data/images <br>\n",
    "https://www.tensorflow.org/guide/keras/preprocessing_layers <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create a dataset using the recommended methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Build a Multilayer Neural Network\n",
    "Now, let's build a multilayer neural network for binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO build a model\n",
    "\n",
    "# TODO compile the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Train the Model\n",
    "Train the model using the Dataset you created\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Evaluate the Model\n",
    "After training, you may upload some test images to evaluate your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "from google.colab import files\n",
    "\n",
    "def load_and_predict(model):\n",
    "    uploaded_files = files.upload()\n",
    "\n",
    "    for fn in uploaded_files.keys():\n",
    "        path = '/content/' + fn\n",
    "        img = image.load_img(path, target_size=(150, 150))\n",
    "        \n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0) / 255.0\n",
    "        \n",
    "        classes = model.predict(x)\n",
    "        result = \"a dog\" if classes[0] > 0.5 else \"a cat\"\n",
    "        \n",
    "        print(f'The model predicts that the image is of {result}')\n",
    "\n",
    "# Call the function to upload images and get predictions\n",
    "load_and_predict(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
